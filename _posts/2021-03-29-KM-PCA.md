---
layout: post
title: Clustering and PCA
subtitle: Unsupervised learning and Dimensionality Reduction
thumbnail-img: /assets/img/posts/2021-03-29-KM-PCA/KM.png
tags: [ML,courses]
use_math: true
---
This week course focuses on Unsupervising Learning and Dimensionality Reduction. For unsupervised learning, ww use k-means clustering algorithm and for dimensionality reduction we use PCA.


```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
from scipy.io import loadmat
import matplotlib.image as mpimg
```

## Problem 1: K-means clustering

### Problem 1.1: Implementing K-means clustering

Finding closest centroid


```python
data = loadmat('./data/ex7data2.mat')
X = data["X"]
print(X.shape)
```

    (300, 2)



```python
def findClosestCentroids(X, centroids):
    K = len(centroids)
    idx = np.zeros(len(X))
    for i in range(len(X)):
        dist = np.sum(np.power(X[i] - centroids, 2), axis=1)
        idx[i] = np.argmin(dist)
    return idx
```


```python
K = 3
initial_centroids = np.array([[3, 3], [6, 2], [8, 5]])
idx = findClosestCentroids(X, initial_centroids)
print("closest centroid for the first 3 examples: ",idx[:3])
```

    closest centroid for the first 3 examples:  [0. 2. 1.]


Now, we can do the second part of k-means which is computing centroid means


```python
def computeCentroids(X, idx, K):
    centroids = np.zeros((K,X.shape[1]))
    for k in range(K):
        centroids[k] = np.mean(X[idx == k], axis=0)
    return centroids
```


```python
centroids = computeCentroids(X, idx, K)
print(centroids)
```

    [[2.42830111 3.15792418]
     [5.81350331 2.63365645]
     [7.11938687 3.6166844 ]]


### Problem 1.2: K-means on example dataset

Finally, we can put both our cluster assignment and cluster computation codes together to iterate between them and come up with the min cust function 


```python
def runkMeans(X, initial_centroids, max_iters):
    K = initial_centroids.shape[0]
    centroids = initial_centroids
    for _ in range(max_iters):
        idx = findClosestCentroids(X, centroids)
        centroids= computeCentroids(X, idx, K)
    return idx, centroids
```


```python
initial_centroids = np.array([[3, 3], [6, 2], [8, 5]])
max_iters = 10
idx, centroids = runkMeans(X, initial_centroids, max_iters)
```


```python
plt.figure()
# plt.scatter(X[idx == 0, 0], X[idx == 0, 1], color='red')
# plt.scatter(X[idx == 1, 0], X[idx == 1, 1], color='green')
# plt.scatter(X[idx == 2, 0], X[idx == 2, 1], color='blue')
sns.scatterplot(x=X[:,0],y=X[:,1], hue = idx.astype(int))
plt.scatter(centroids[:, 0], centroids[:, 1], color='red', marker="+", s=200)
plt.show()
```


    
![png](/assets/img/posts/2021-03-29-KM-PCA/Ex_7_16_0.png)
    


### Problem 1.3: Random initialization

The problem with the minimizatin of the cost funciton above, is that we selected a prefered initial centroid value. Because of this, we might be stuk in a local minimum. This is why we must use random init to randomize the selection of our initial centroids.


```python
def kMeansInitCentroids(X, K):
    rng = np.random.RandomState(0)
    idx = np.arange(X.shape[0])
    rng.shuffle(idx)
    centroids = X[idx[:K]]
    return centroids
```

### Problem 1.4: Image compression with K-means

In this exercise, we will apply K-means to image compression. In a straightforward 24-bit color representation of an image, each pixel is represented as three 8-bit unsigned integers (ranging from 0 to 255) that specify the red, green and blue intensity values. This encoding is often refered to as the RGB encoding. Our image contains thousands of colors, and in this part of the exercise, we will reduce the number of colors to 16 colors. By making this reduction, it is possible to represent (compress) the photo in an efficient way. Specifically, we only need to store the RGB values of the 16 selected colors, and for each pixel in the image you now need to only store the index of the color at that location (where only 4 bits are necessary to represent 16 possibilities). 


```python
plt.figure()
plt.imshow(mpimg.imread('./data/bird_small.png'))
plt.xticks([])
plt.yticks([])
plt.show()
```


    
![png](/assets/img/posts/2021-03-29-KM-PCA/Ex_7_22_0.png)
    



```python
A = mpimg.imread('./data/bird_small.png')
A = A/225
print(A.shape)
```

    (128, 128, 3)



```python
X = A.reshape(A.shape[0] * A.shape[1], A.shape[2])
X.shape
```




    (16384, 3)




```python
K = 16
max_iters = 10
initial_centroids = kMeansInitCentroids(X, K)
idx, centroids = runkMeans(X, initial_centroids, max_iters)
```


```python
X_recovered = centroids[findClosestCentroids(X, centroids).astype(int)]
X_recovered = X_recovered.reshape(A.shape[0], A.shape[1], A.shape[2])
print(X_recovered.shape)
```

    (128, 128, 3)



```python
plt.figure()
plt.subplot(121)
plt.imshow(A*225)
plt.xticks([])
plt.yticks([])
plt.title("Original")
plt.subplot(122)
plt.imshow(X_recovered*225)
plt.xticks([])
plt.yticks([])
plt.title("Compressed with 16 colors")
plt.show()
```


    
![png](/assets/img/posts/2021-03-29-KM-PCA/Ex_7_27_0.png)
    


## Problem 2: PCA

### Problem 2.1: Example dataset


```python
data2 = loadmat('./data/ex7data1.mat')
X = data2["X"]
print(X.shape)
```

    (50, 2)



```python
plt.figure()
sns.scatterplot(x=X[:,0],y=X[:,1])
plt.show()
```


    
![png](/assets/img/posts/2021-03-29-KM-PCA/Ex_7_31_0.png)
    


### Problem 2.2: Implementing PCA

First we want to do normalization


```python
def featureNormalize(X):
    mean = np.mean(X, axis=0)
    std = np.std(X, ddof=1, axis=0)
    X_norm = (X - mean) / std
    return X_norm, mean, std
```

Then, we can define a function for PCA that will output U and S so we can then get z


```python
def pca(X):
    sigma = np.dot(X.T, X) / X.shape[0]
    U, S, _ = np.linalg.svd(sigma)
    return U, S
```


```python
X_norm, mean, std = featureNormalize(X)
U, S = pca(X_norm)
```


```python
plt.figure(figsize=(4,4))
plt.scatter(X[:, 0], X[:, 1])
plt.plot([mean[0], mean[0] + 1.5 * S[0] * U[0, 0]],
         [mean[1], mean[1] + 1.5 * S[0] * U[1, 0]],
         color="black", linewidth=4)
plt.plot([mean[0], mean[0] + 1.5 * S[1] * U[0, 1]],
         [mean[1], mean[1] + 1.5 * S[1] * U[1, 1]],
         color="black", linewidth=4)
plt.show()
```


    
![png](/assets/img/posts/2021-03-29-KM-PCA/Ex_7_38_0.png)
    


Now that we know our PCA's output outputs the correct eigenvalue, we can do dimensionality red

### Problem 2.3: Dimensionality reduction with PCA


```python
def projectData(X, U, K):
    Ureduce = U[:, :K] #from n-dim (2D) to K-dim (1D)
    z = np.dot(X, Ureduce)
    return z
```


```python
K = 1
z = projectData(X_norm, U, K)
print('Projection of the first example:', z[0])
```

    Projection of the first example: [1.48127391]


We can do it backwards and "reconstruct" or X with the optimal fitted line from our PCA function


```python
def recoverData(z, U, K):
    Ureduce = U[:, :K]
    X_rec = np.dot(z, Ureduce.T)
    return X_rec
```


```python
X_rec  = recoverData(z, U, K)
print('Approximation of the first example:', X_rec[0,:])
```

    Approximation of the first example: [-1.04741883 -1.04741883]



```python
plt.figure()
plt.scatter(X_norm[:, 0], X_norm[:, 1], label = "Original Data")
plt.scatter(X_rec[:, 0], X_rec[:, 1], label = "Recovered Data")
plt.legend()
plt.show()
```


    
![png](/assets/img/posts/2021-03-29-KM-PCA/Ex_7_46_0.png)
    


### Problem 2.4: Face image Dataset


```python
data3 = loadmat('./data/ex7faces.mat')
X = data3["X"]
print(X.shape)
```

    (5000, 1024)



```python
plt.figure()
for i in range (100):
    plt.subplot(10,10,i+1)
    plt.imshow(X[i,:].reshape((32, 32)).T,
               cmap=plt.cm.bone, interpolation='nearest')
    plt.xticks([])
    plt.yticks([])
plt.show()
```


    
![png](/assets/img/posts/2021-03-29-KM-PCA/Ex_7_49_0.png)
    


Now, lets do PCA for faces and do a dimensionality reduction from 1024-dim to 100-dim


```python
X_norm, mean, std = featureNormalize(X)
U, S = pca(X_norm)
```




    (1024, 1024)



Visualize the first 36 eigenvectors


```python
plt.figure()
for i in range (36):
    plt.subplot(6,6,i+1)
    plt.imshow(U[:,i].reshape((32, 32)).T,
               cmap=plt.cm.bone, interpolation='nearest')
    plt.xticks([])
    plt.yticks([])
plt.show()
```


    
![png](/assets/img/posts/2021-03-29-KM-PCA/Ex_7_53_0.png)
    



```python
K = 100
z = projectData(X_norm, U, K)
print('The projected data z has a size of:', z.shape)
```

    The projected data z has a size of: (5000, 100)



```python
X_rec  = recoverData(z, U, K)
```


```python
plt.figure()
plt.subplot(121)
plt.imshow(X_norm[0,:].reshape((32, 32)).T,
               cmap=plt.cm.bone, interpolation='nearest')
plt.xticks([])
plt.yticks([])
plt.title("Original")
plt.subplot(122)
plt.imshow(X_rec[0,:].reshape((32, 32)).T,
               cmap=plt.cm.bone, interpolation='nearest')
plt.xticks([])
plt.yticks([])
plt.title("Reconstructed PCA K=100 ")
plt.show()
```


    
![png](/assets/img/posts/2021-03-29-KM-PCA/Ex_7_56_0.png)
    


## Problem 2.5: PCA for Viz

Load the same data from the Small bird problem


```python
A = mpimg.imread('./data/bird_small.png')
A = A/225
print(A.shape)
```

    (128, 128, 3)



```python
X = A.reshape(A.shape[0] * A.shape[1], -1)
print(X.shape)
```

    (16384, 3)



```python
K = 16
max_iters = 10
initial_centroids = kMeansInitCentroids(X, K)
idx, centroids = runkMeans(X, initial_centroids, max_iters)
```

Now, instead of k=16, we setup k=2 so we can plot in 2D


```python
K = 2
X_norm, mu, sigma = featureNormalize(X)
U, S = pca(X_norm)
z = projectData(X_norm, U, K)
print(z.shape)
```

    (16384, 2)



```python
plt.figure()
plt.scatter(z[:, 0], z[:, 1], c=idx)
plt.show()
```


    
![png](/assets/img/posts/2021-03-29-KM-PCA/Ex_7_64_0.png)
    

