---
layout: post
title: Fastai - Practical DL for Coders
subtitle: Best Deep Learning MOOC?
thumbnail-img: /assets/img/posts/2021-05-20-fastai/logo.png
tags: [DL,courses]
use_math: true
---

After finishing Andrew Ng's famous ML course, I felt that I needed more practical training and SOTA results so I went and did Fastai Practical Deep Learning for Coders MOOC taught by Jeremy Howard. 

I was undecided if I wanted to do something similar as I did in Andrew Ng's course by posting each assignment as a blog, but I decided not to since it didn't add any value because all the code was already given in the course. At the end of the day I thought the best solution was to give a brief review and compare it to the Andrew's course.

![png](/assets/img/posts/2021-05-20-fastai/fastai.png)

## Deep Learning for Coders with Fastai

This course is based on this <a target="_blank" href="https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527">book</a> and each lesson is based on a chapter of the book. The book is also avaidable as Jupyter Notebooks for free <a target="_blank" href="https://github.com/fastai/fastbook">here</a>. If you prefer, you could easily go through the book and not listen to any lectures, you will get the same out of the course. The only advantage of going through the lectures (as any college course) is to learn some cool tips and tricks from the instructor (Jeremy). Jeremy is a very succesful AI entrepenuer and you will get a glimpse of how he thinks and why he is as succesfull as he is if you listen to the lectures. 

Now, I was coming from Andrew Ng's ML course, where he perfectly explained how everything worked from beggining to end. In this course, Jeremy uses a technique where he explaines everything from end to beggining. In the first lesson you are already training SOTA text and vision models and in the second lesson you put in production a SOTA vision classification model. What a refreshing change of pace! Seeing these results so quickly motivated me to keep going and not losing focus. 

![png](/assets/img/posts/2021-05-20-fastai/bear.png)

After doing very high level stuff with fastai library in the first couple of lessons, the lessons start diving deep into concepts such as building a neural net from scratch to use with vision classification data. Same as with Andrew's ML course, fastai course utilizes de MNIST dataset to explain basic concepts of classification. Here is where I start noticing different methodologies used now in 2020 vs those used in 2011 (when Andrew Ng's ML course was released). In Andrew's ML course, When doing multinomial classification problems, we used one-vs-all methodology as a classifier. This meant treating the problem as a binary classification problem. Now in 2020, a more optimal way of doing this is by using a multi-class classifier such as softmax. Softmax will take any number of labels and calculate the probability of each beign correct. This works well in the Deep Learning era since we have so much data and labels. You can see <a target="_blank" href="https://rud0812.github.io/2021-03-21-NN/">here</a> a blog post explaining the MNIST one-vs-all assignment from Andrew's ML course and <a target="_blank" href="https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb">here</a> for the book chapter containing the code from Deep Learning for Coders course.

Now, you are 6 lessons in and have mostly seen only how vision models work. At this point, you can easily start taking on some Kaggle competitions on vision because there is no more vision in the remainind lessons. Keep in mind that there are more chapters in the book than there are lessons, so if you want to go even deeper and build a CNN from scratch, you could go and look at the book's <a target="_blank" href="https://github.com/fastai/fastbook/blob/master/13_convolutions.ipynb">Chapter 13</a>. Jeremy explains that for just building models and doing Kaggle competitions, you don't need to know how to build CNN from scratch, this is more usefull if you are a researcher or trying to come up with better SOTA results, which in practice is not needed.

Next lessons is all about collaborative filtering for recommender systems. It also goes slightly over PCA as a dimensionality reduction method to better explain our results. This lesson was extremly fun and insightfull. Jeremy introduces embedding matrices, which as you might already now are the bread and butter of deep learning. He also shows how to build recommender systems with both dot product (even from excel lol) and neural networks and why the SOTA results are almost the same using both methodologies.

![png](/assets/img/posts/2021-05-20-fastai/colab.png)

The next 2 and final lessons covered are Tabular Modeling and NLP. I'm not going to go through tabular modeling since I didn't find it usefull at all. I think you are better off using sklearn from scratch to build random forests, which is pretty straight forward. Another solution for SOTA results (which is not covered in the book), is to use XGBoost for gradient boosted trees. Now for NLP, the last lesson covered two chapters of the book. A basic NLP intro and a deep dive to building an NLP model from scratch using LSTM. It saddens me that Jeremy didn't covered new techniques such as attention trasnformers in this course. Fastai has another <a target="_blank" href="https://www.fast.ai/2019/07/08/fastai-nlp/">course</a> that especialicies in NLP, which he basically implements the famous "Attention is all you need" paper with fastai library.

### Comparison table against Andrew Ng's ML course:

<!-- TABLE_GENERATE_START -->
|DL for Coders                                                         | ML  	                                                                |
|----------------------------------------------------------------------|-----------------------------------------------------------------------|
|Production in a week                                                  |No production at all                                                   |
|Teaches you SOTA models very quickly  	                               |Only teaches theory and very basic implementation  	                   |
|Some crucial math is left behind for the user to understant like SGD  |Everything is explained intensively, specially gradient descent and other things such as PCa  	                                           |
|It's difficult to know where fastai starts and Pytorch ends           |Everything is done by hand in MATLAB (or Python like I did, suing Numpy)  	                                                             |
|Jeremy is a godd instructor  	                                       |Andrew is an even better instructor                                    |
<!-- TABLE_GENERATE_END -->

## Final Review
I think Deep Learning for Coders with Fastai is an incredible course to get started with deep learning but I highly recommend knowing some machine learning beforehand. While it is definetly not needed to complete the course, it is extremly usefull to understand most of the topics, math and stats that Jeremy skips. This is the main reason I recommend people to take Andrew Ng's 2011 ML course from Coursera. You build a great foundation, specially if you do all assignments in Python.

My next steps after these 2 courses will be:
- Start doing Kaggle competitions on vision and text to deepend my understanding of fastai and Pytorch. 
- Get a better grasp on computer science theory by taking CS50.
- After training at least 10 models in Kaggle and completting CS50, I'll take a look at Full Stack Deep Learning MOOC to learn how to build large scale production models.

Stay tunned for more!


